\subsection{Stream Classification}
Data stream classification is a challenging task due to its inherent properties, such as infinite length and concept drift. The infinite length problem is addressed by dividing the whole stream into fixed-size minibatches~\cite{conf/icdm/MasudGKHT08} or using a gradual forgetting mechanism~\cite{journals/ida/Klinkenberg04}. Recent approaches~\cite{conf/icde/HaqueKBTA16,conf/sdm/BifetG07} address this problem by remembering only the instances within two consecutive concept drifts using a dynamically-sized minibatch. The minibatch size is increased until a concept drift is detected. Once a drift is detected, the classifier is updated and the instances representing the old concept in the minibatch are removed.

Concept drift detection in multivariate data streams concentrates on tracking any changes in the posterior class distribution, $P(y|x)$. Instead of tracking changes in $P(y|x)$ directly, the approaches proposed in~\cite{conf/sbia/GamaMCR04} adopt the principle~\cite{books/daglib/0097035} to detect this change indirectly by tracking drift in the error rate of the underlying classifier. However, tracking drift in the error rate requires true labels of test data instances, which are scarce in practice. Recent studies focus on a feedback mechanism using prediction error~\cite{conf/sbia/GamaMCR04} or confidence~\cite{conf/cikm/ChandraHKA16} to detect changes in distribution between two different time windows by detecting change points in the process. Apart from working on a single stream, these methods explicitly detect change points.

The problem setting described in this paper is motivated by a Multistream Classification framework ~\cite{conf/cikm/ChandraHKA16}. It proposes a solution which focuses on the classification problems when there are asynchronous concept drifts on source and target streams. Since stream classification is a continuous process, data in the target stream is assumed to be generated with very few labels. However, there are two strong assumptions made for the proposed solutions: First, there are fixed number of features in both source and target streams; Next, features have a strict one to one mapping/correspondence from source stream to target stream. However, these assumptions may not  always hold in real world scenarios. To overcome those obstacles, our proposed approach does not have this later strong assumptions. Our proposed approach assumes that features are not exactly the same between domains, both in number and correspondence. However, other features in source stream may not have mapping to features in target stream and vice-versa. 

\subsection{Data Domain Adaptation}

A fundamental assumption in data mining, known as the ``stationary distribution assumption'', is that both the training and test data are generated from the same domain and thus represent the same data distribution~\cite{conf/icml/Zadrozny04}. Therefore, these techniques cannot be directly employed when such an assumption is not valid. Instance weighting is a frequently used technique, where a weight is associated with each training data point and used in the learning process. KMM~\cite{conf/nips/HuangSGBS06}, KLIEP~\cite{artical:sugiyama2008direct}, and uLSIF~\cite{journals/jmlr/KanamoriHS09} are among the techniques that are available in the literature for handling data shift. However, these approaches all assume that the training and test data are gained from the same domain and share the same feature space. Thus, a new strategy is needed to adapt those two different but somehow related data into a common latent feature space.

Recently, several approaches have been proposed to learn a common feature representation for domain adaptation~\cite{journals/ml/Ben-DavidBCKPV10,journals/tkde/PanY10,journals/jbd/DayK17}. \cite{conf/acl/Daume07} proposed a simple heuristic nonlinear mapping function to map the data from both source and target domains to a high- dimensional feature space. \cite{conf/aaai/PanKY08,journals/tnn/PanTKY11} utilize a new dimensionality reduction method, Maximum Mean Discrepancy Embedding (MMDE), for domain adaptation. They try to learn a set of common transfer components underlying both domains such that the difference in distributions of data in the different domains, when projected onto this subspace, can be reduced.

Similarly, as we stated in the last section, it is challenging to derive an online method to reduce the distribution different between domains. The state-of-the-art methods mostly address domain adaptation problems for stationary data. Thus, fitting domain adaptation frameworks to online data streams is yet to explore.
