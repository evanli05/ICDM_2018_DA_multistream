There are mainly two research aspects that are related to our proposed model. 
The first one is domain adaptation, the other one is concept drift correction. The following reviews some important related work.

\subsection{Domain Adaptation}

One of fundamental assumptions in data mining, known as the ``stationary distribution assumption'', is that both the training and test data are generated from the same domain and thus represent the same data distribution~\cite{zadrozny2004learning}. Therefore, common techniques normally cannot be directly deployed when training and test data are from different domains. The differences between domains can be normally considered as two aspects: 1. distinct number of features; 2. distinct feature distributions. Several approaches have been proposed to learn a common feature representation for domain adaptation~\cite{journals/ml/Ben-DavidBCKPV10,pan2010survey}. Pan et al. ~\cite{conf/aaai/PanKY08} utilize a new dimension reduction method, Maximum Mean Discrepancy Embedding (MMDE), for domain adaptation. They try to learn a set of common transfer components underlying both domains such that the difference in distributions of data in the different domains, when projected onto this subspace, can be reduced. Shi et al.~\cite{shi2010transfer} proposed an linear objective function to project a latent feature space for both source and target domains. Both of these approaches~\cite{shi2010transfer, conf/aaai/PanKY08} focus on  static data only. Recall that in stream feature space may evolve over times across streams, which may trigger heterogeneous feature space. Zhao et al. ~\cite{zhao2010otl} uses a co-regularized method to project target domain to source domain in an online manner. However, this method also makes a strong assumption that features in source dataset is a subset of those in target dataset, which in reality, this is not always the case. However, the method works in an incremental manner which may be suitable for stream setting.

Similarly, as we stated in the last section, it is challenging to derive an online method to reduce the distribution different between domains. The state-of-the-art methods mostly address domain adaptation problems for stationary data except Zhao et al~\cite{zhao2010otl}. Thus, fitting domain adaptation frameworks to online data streams is yet to explore.



\subsection{Stream Classification}
Data stream classification is a challenging task due to its inherent properties, such as infinite length and concept drift. The infinite length problem is addressed by dividing the whole stream into fixed-size minibatches or using a gradual forgetting mechanism~\cite{journals/icdm/MasudGKHT08}. Recent approach~\cite{conf/sdm/BifetG07} address this problem by remembering only the instances within two consecutive concept drifts using a dynamically-sized minibatch. The minibatch size is increased until a concept drift is detected. Once a drift is detected, the classifier is updated and the instances representing the old concept in the minibatch are removed.

Concept drift detection in multivariate data streams concentrates on tracking any changes in the posterior class distribution, $P(y|x)$. Instead of tracking changes in $P(y|x)$ directly, the approaches proposed in~\cite{conf/sbia/GamaMCR04} adopt the principle~\cite{haque2018framework} to detect this change indirectly by tracking drift in the error rate of the underlying classifier. However, tracking drift in the error rate requires true labels of test data instances, which are scarce in practice. Recent studies focus on a feedback mechanism using prediction error~\cite{conf/sbia/GamaMCR04} or confidence~\cite{chandra2016adaptive} to detect changes in distribution between two different time windows by detecting change points in the process. Apart from working on a single stream, these methods explicitly detect change points.

The problem setting described in this paper is motivated by a Multistream Classification framework~\cite{chandra2016adaptive}. It proposes a solution which focuses on the classification problems when there are asynchronous concept drifts on source and target streams. Since stream classification is a continuous process, data in the target stream is assumed to be generated with very few labels. Yet, there are two strong assumptions made for this work: First, there is a fixed number of features in both source and target streams. Next, features have a strict one to one mapping/correspondence from source stream to target stream. However, these assumptions may not always hold in real world scenarios. To overcome those obstacles, our proposed approach does not have this latter strong assumptions. Our proposed approach assumes that domains are related (e.g., Video review vs DVD review domains), however,  features may not be exactly the same between domains, both in number and correspondence. Furthermore, other features in source stream may not have any mapping to features in target stream and vice-versa. 


